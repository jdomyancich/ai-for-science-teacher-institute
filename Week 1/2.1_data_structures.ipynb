{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Pandas and DataFrames\n",
    "\n",
    "Pandas is a powerful Python library that is essential for data science and analysis. It provides high-performance, easy-to-use data structures, the most important of which is the DataFrame. You can think of a DataFrame as a smart, programmable spreadsheet, like Excel or Google Sheets, but with the full power of Python behind it.\n",
    "\n",
    "Here we'll look into the structure of a dataframe and walk through the most critical first steps to take immediately after loading a dataset into a pandas as a dataframe. Think of this as the initial \"health check\" for your data, a fundamental skill for any data analyst or scientist.\n",
    "\n",
    "Why is this so important? Before you can perform any meaningful analysis, build a model, or create a visualization, you must first understand the data you're working with.  Mastering these first few commands builds a solid foundation for all your future data work and prevents common errors down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (cont.)\n",
    "\n",
    "We have already seen a couple examples of **structured data**: lists and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "Lists are **ordered**, meaning the items are labeled using an **index**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_list = [\"eggs\", \"milk\", \"juice\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Arrays are a special type of list that contains only one data type. This homogeneity allows for more efficient memory usage and faster numerical computations, making them a favorite in scientific computing. Arrays are typically more rigid than lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which list is an array?\n",
    "planet_list = ['mercury', 'venus', 'earth', 'mars', 'jupiter']\n",
    "age_list = [3, 8, 12.0, 38, 40]\n",
    "number_list = [4, 'five', 'ten', 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaway for both lists and arrays is their integer-based indexing. You retrieve an element by its numerical position, starting from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tabular Powerhouse: DataFrames\n",
    "Enter the dataframe, a structure that brings together the concepts of lists, arrays, and dictionaries to create a powerful tool for data analysis. A dataframe is a two-dimensional data structure with labeled rows and columns. Imagine a spreadsheet, and you have a good mental model of a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing the Pandas Library\n",
    "\n",
    "In order to have access to the functions in a library, we have to \"import\" it. This is done with a simple line of code. A common practice it to:\n",
    "\n",
    "`import <library_name> as <abbreviated_name>`. \n",
    "\n",
    "This allows us to define an abbreviation for the library to use whenever we call a function from it instead of typing out the full name of the library every time. The abbreviation is up to you, but most libraries have an agreed upon abbreviation. For examples, most people abbreviate pandas as `pd`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas dataframe should look familiar. It is how data is structured in spreadsheets like Excel and Google Sheets. The bold numbers on the left are known as the **index** and allow us to number rows. The column names allow us to label and reference individual arrays in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading in Data to a DataFrame a from .csv File\n",
    "\n",
    "It is possible to create a dataframe from scratch, but we usualliy will bring in data from an existing file, also known as a **dataset**. The most common file format for raw data is a `.csv` file, short for \"comma separated values\" and it is very easy to convert it into a dataframe:\n",
    "```\n",
    "Planet,Order,Moons\n",
    "Mercury,1,0\n",
    "Venus,2,0\n",
    "Earth,3,1\n",
    "Mars,4,2\n",
    "Jupiter,5,79\n",
    "```\n",
    "\n",
    "`read_csv()` takes a file path as its argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding File Paths for Reading CSVs with Pandas\n",
    "\n",
    "When you use `pd.read_csv('filename.csv')`, you're telling pandas to find a file and load it into a DataFrame. The string you provide inside the parentheses is the **file path**. It's the set of directions from your current location to the file you want.\n",
    "\n",
    "See the \"Supplementary Information\" section for more on file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a .csv file called `planets.csv` in the \"data\" folder that we can load into pandas and display as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Datasets with Pandas\n",
    "\n",
    "While we can load small datasets like `planets.csv` into Pandas, this library shines when dealing with large datasets. Now we will look at an authentic dataset of historical global temperatures, courtesy of [Berkeley Earth](https://berkeleyearth.org/about/), which is affiliated with [Lawrence Berkeley National Laboratory](https://www.lbl.gov/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a First Look\n",
    "\n",
    "Once you've loaded your dataset into a pandas DataFrame using `pd.read_csv()`, the crucial next step is to get a feel for your data. This initial inspection helps you understand its structure, identify potential issues like missing values, and get a sense of the data's content.\n",
    "\n",
    "Here are the essential dataframe methods and attributes you should use right away. Remember, a dataframe is an **object**, meaning it has certain **methods** associated with it. In pandas (and Python in general), an **attribute** is a piece of data or a characteristic stored on an object, which you access directly by name. Think of it as a property of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.head()` - View the first few rows\n",
    "This is the most common first step. It shows you the first 5 rows of your DataFrame by default, giving you an immediate look at your columns and the type of data they contain.\n",
    "\n",
    "**Why it's useful**: It's a quick way to confirm your data loaded correctly and to see the column names and data examples without printing the entire (potentially massive) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.tail()` - View the last few rows\n",
    "Similar to `.head()`, this method shows you the last 5 rows of your DataFrame.\n",
    "\n",
    "**Why it's useful**: This can help you spot any issues with the end of your file, such as summary rows or trailing blank data that might have been loaded incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.shape` - Check the dimensions\n",
    "This is an attribute (not a method, so no parentheses) that returns a tuple representing the dimensions of the DataFrame: `(number_of_rows, number_of_columns)`. \n",
    "\n",
    "**Why it's useful**: It tells you exactly how big your dataset is, which is fundamental information for any further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.info()` - Get a technical summary\n",
    "\n",
    "This method provides a concise technical summary of the dataframe. It's one of the most valuable inspection tools.\n",
    "\n",
    "**Why it's useful**:\n",
    "- Index type: Shows the index type and range.\n",
    "- Column count: Confirms the total number of columns.\n",
    "- Dtype: Shows the data type (e.g., `int64`, `float64`, `object`) for each column. An object dtype often means the column contains strings.\n",
    "- Memory usage: Gives you an idea of how much memory the dataframe is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.describe()` - Generate descriptive statistics\n",
    "\n",
    "This method generates descriptive statistics for the numerical columns in your dataframe.\n",
    "\n",
    "**Why it's useful**: It provides a quick overview of the distribution, central tendency, and spread of your numerical data. You can quickly spot things like outliers (e.g., a huge max value) or unexpected distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.columns` - View column names\n",
    "\n",
    "This attribute returns a list of all the column names in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## ðŸ’ª **Exercise** ðŸ’ª\n",
    "\n",
    "1. Load the `nba_player_stats.csv` file into a pandas dataframe from the `data` folder.\n",
    "2. Inspect the dataframe: Use the methods and attributes described above to inspect your dataframe and answer the following questions:\n",
    "- What are the column names?\n",
    "- How many entries (rows) are there?\n",
    "- How are the entries in the dataframe ordered?\n",
    "- What is the record for most points scored in a season?\n",
    "- On average, how many years experience does an NBA player have?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Wrangling\n",
    "\n",
    "This is one of the hardest parts of working with data. Raw data is rarely ready to be analyzed. Scientists spend a lot of time manipulating their data to get it into a form that can be used. We call it \"wrangling\".\n",
    "\n",
    "<img src = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Cats_in_aoshima_island_1.JPG/1600px-Cats_in_aoshima_island_1.JPG' width = 400>\n",
    "\n",
    "Possible problems with data:\n",
    "* Getting only the data you need \n",
    "* Incorrect data types\n",
    "* Incorrect units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns\n",
    "\n",
    "The most common way to get data from a DataFrame is by selecting one or more columns. Remember, each column is an array, meaning all the elements in a column are the same datatype (or they should be).\n",
    "\n",
    "We'll continue using the historical global temperatures dataset from [Berkeley Earth](https://berkeleyearth.org/about/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Single Column\n",
    "To select a single column, use square bracket notation with the column name as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Multiple Columns\n",
    "\n",
    "To select multiple columns, you need to pass a **list** of column names inside the square brackets. This will return a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done in a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Filtering (Boolean Indexing)\n",
    "\n",
    "This is one of the most powerful features of Pandas: selecting rows based on whether they meet a certain condition. You pass a Series of `True`/`False` values (a \"Boolean Series\") inside the square brackets. Only rows where the value is `True` will be returned. The easiest way to undrstand this is by breaking it down, step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Create a condition**\n",
    "\n",
    "Let's ask a question: \"Which cities have experienced very hot average temperature for a specific month?\" Let's say 35&deg;C (95&deg;F) is \"very hot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Generate the Boolean Series**\n",
    "\n",
    "If you `print(hot_cities)`, you wouldn't see the cities. You would see the Boolean Seriesâ€”the `True`/`False` answer for every single city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Use the Boolean Series to filter the dataframe**\n",
    "Now, you use that condition (the Boolean Series) as a filter inside the square brackets of your original dataframe.\n",
    "\n",
    "Pandas will look at your `global_temps` dataframe, and for every row, it will check the corresponding value in your condition series. It will only keep the rows where the value is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Conditions\n",
    "\n",
    "You can combine multiple conditions using logical operators:\n",
    "\n",
    "- & for AND (both conditions must be true)\n",
    "- | for OR (at least one condition must be true)\n",
    "\n",
    "Remember to put each condition in parentheses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this can be conbined into a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## ðŸ’ªðŸ’ª **Pair Programming Exercise** ðŸ’ªðŸ’ª\n",
    "\n",
    "Partner with someone and do these 2 problems together on 1 computer. The first person \"drives\" for problem 1, the second for problem 2.\n",
    "\n",
    "Using the original `global_temps` dataframe:\n",
    "\n",
    "1. Find all the records in the `global_temps` dataframe where the country is `United States`. Create a new dataframe called `us_temps` to store this data.\n",
    "2. Now, let's find the coldest months on record in Chicago. Filter the `us_temps` dataframe to find all records for the City of 'Chicago' where the `AverageTemperature` was below -5Â°C.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ““ Reflection ðŸ““\n",
    "\n",
    "The data we saw here could also be viewed and inspected with a spreadsheet appplication like Excel or Google Sheets. What are the advantages and disadvantages of these each approach of the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Dictionaries and DataFrames\n",
    "Enter the dataframe, a structure that brings together the concepts of lists, arrays, and dictionaries to create a powerful tool for data analysis, most notably implemented in the Pandas library in Python. A dataframe is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns). Imagine a spreadsheet, and you have a good mental model of a dataframe.\n",
    "\n",
    "The relationship between dictionaries and dataframes is direct and fundamental. One of the most common ways to create a dataframe is from a dictionary. There are two primary methods. Which do you prefer?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dictionary of Lists**: You can create a dictionary where the keys represent the column names and the values are lists or arrays of the data for each column. This structure maps directly to the columnar nature of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_planets = {\n",
    "    'Planet': ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter'],\n",
    "    'Order': [1, 2, 3, 4, 5],\n",
    "    'Number of Moons': [0, 0, 1, 2, 79]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **List of Dictionaries:** Alternatively, you can have a list where each element is a dictionary representing a row of data. In this case, the keys of each dictionary become the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_planets = [{\"Planet\": \"Mercury\", \"Order\":1, \"Number of Moons\":0},\n",
    "                  {\"Planet\": \"Venus\", \"Order\": 2, \"Number of Moons\": 0},\n",
    "                  {\"Planet\": \"Earth\", \"Order\": 3, \"Number of Moons\": 1},\n",
    "                  {\"Planet\": \"Mars\", \"Order\": 4, \"Number of Moons\": 2},\n",
    "                  {\"Planet\": \"Jupiter\", \"Order\": 5, \"Number of Moons\": 79}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the proper dictionary (or dictionaries), you can convert it to a Pandas DataFrame using the `DataFrame` function form the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planets_df = pd.DataFrame(pandas_planets)   # df is a common abbreviation for \"dataframe\"\n",
    "print(planets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also skip the `print()` function when displaying a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas dataframe should look familiar. It is how data is structured in spreadsheets like Excel and Google Sheets. The bold numbers on the left are known as the **index** and allow us to number rows. The column names allow us to label and reference individual arrays in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To `print` or not to print?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | `dataframe_name` (in a cell) | `print(dataframe_name)` |\n",
    "|:---|:---|:---|\n",
    "| **Output Format** | Richly formatted HTML table | Plain text |\n",
    "| **Readability** | High, visually organized | Lower, less structured |\n",
    "| **Underlying Call** | Implicit `IPython.display.display()` | Standard `print()` which calls `__str__()` |\n",
    "| **Environment** | Primarily for Jupyter/IPython | Works in any Python environment |\n",
    "| **Cell Behavior** | Only works for the *last* expression in a cell | Can be used anywhere in a cell to print multiple outputs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Understanding File Paths for Reading CSVs with Pandas\n",
    "\n",
    "When you use `pd.read_csv('filename.csv')`, you're telling pandas to find a file and load it into a DataFrame. The string you provide inside the parentheses is the **file path**. It's the set of directions from your current location to the file you want.\n",
    "\n",
    "There are two fundamental types of file paths: relative and absolute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Type 1: Relative Paths (Most Common)\n",
    "A relative path describes the location of a file relative to your current working directory. The \"current working directory\" is the folder where your Python script or Jupyter Notebook is currently running. This is the most common and portable way to load local files.\n",
    "\n",
    "**Scenario A: The CSV is in the SAME directory as your notebook**\n",
    "This is the simplest case. You just need to provide the filename.\n",
    "\n",
    "Directory Structure:\n",
    "```\n",
    "/MyProject\n",
    "â”œâ”€â”€ my_notebook.ipynb\n",
    "â””â”€â”€ planets.csv\n",
    "```\n",
    "\n",
    "Code:\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('planets.csv')\n",
    "```\n",
    "\n",
    "**Scenario B: The CSV is in a SUBDIRECTORY (a folder inside your current folder)***\n",
    "You need to specify the folder name, followed by a slash, then the filename.\n",
    "\n",
    "Directory Structure:\n",
    "```\n",
    "/MyProject\n",
    "â”œâ”€â”€ my_notebook.ipynb\n",
    "â””â”€â”€ /data\n",
    "    â””â”€â”€ planets.csv\n",
    "```\n",
    "Code:\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/planets.csv')\n",
    "```\n",
    "\n",
    "**Scenario C: The CSV is in a PARENT DIRECTORY (the folder containing your current folder)**\n",
    "You use `..` to go \"up\" one directory level.\n",
    "\n",
    "Directory Structure:\n",
    "```\n",
    "/MyProject\n",
    "â”œâ”€â”€ /notebooks\n",
    "â”‚   â””â”€â”€ my_notebook.ipynb\n",
    "â””â”€â”€ /data\n",
    "    â””â”€â”€ planets.csv\n",
    "```\n",
    "\n",
    "If your current working directory is /notebooks, you would go up one level to /MyProject, then down into /data.\n",
    "\n",
    "Code:\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/planets.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type 2: Absolute Paths\n",
    "An absolute path specifies the location of a file starting from the very root of your computer's file system. These paths are long and specific to your machine, which makes them less portable. You should generally avoid them unless necessary.\n",
    "\n",
    "On Windows an absolute path typically starts with a drive letter, like `C:`. The 'r' before the string is important! It creates a \"raw\" string which tells Python to ignore the special meaning of backslashes.\n",
    "\n",
    "`df = pd.read_csv(r'C:\\Users\\YourUsername\\Documents\\MyProject\\data\\planets.csv')`\n",
    "\n",
    "MacOS or Linux starts from the root directory, which is a single forward slash (/).\n",
    "\n",
    "`df = pd.read_csv('/home/yourusername/documents/MyProject/data/planets.csv')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Tips & Best Practices\n",
    "\n",
    "- **Check Your Location**: If you're ever unsure about your current working directory, you can find out with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prefer Relative Paths**: They make your code portable. If you use relative paths and send your project folder to someone else, your code will still work on their machine.\n",
    "\n",
    "- **Use Forward Slashes (/)**: Pandas understands forward slashes on all operating systems, making your code more universal.\n",
    "\n",
    "- **Avoid Spaces in Filenames**: While possible to handle, spaces in file and folder names can sometimes cause issues. Using underscores (_) or hyphens (-) is a safer convention (e.g., my_data_file.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Selecting Rows\n",
    "\n",
    "Rows are a little trickier to reference because they are not arrays. In columns, we can reference the name of the columns that we are interested in. Also, the elements in a column are the same data type. Since rows contain the elements of several different arrays, they can contain multiple data types and require a different way of thinking. We can filter rows two ways: by index or by label.\n",
    "\n",
    "- `loc` (label-based indexing): Used for selecting by the actual labels of your index (which are often numbers, but can be text or dates).\n",
    "\n",
    "- `iloc` (integer-location based indexing): Used for selecting by the numerical position of the rows and columns (always 0-indexed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Rows with `loc`\n",
    "\n",
    "`loc` works with the index labels. If your DataFrame has a default integer index (0, 1, 2...), `loc` can use these integers. You can select by:\n",
    "- single rows\n",
    "- multiple rows\n",
    "- range of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row\n",
    "first_row = city_temps.loc[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple rows\n",
    "mult_row = city_temps.loc[[2, 343, 10555]]\n",
    "mult_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of rows\n",
    "range_row = city_temps.loc[502:508]\n",
    "range_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Rows with `iloc`\n",
    "\n",
    "`iloc` works with the position of the row/column, always starting from 0. You can select by:\n",
    "- single rows\n",
    "- multiple rows\n",
    "- range of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row\n",
    "first_row = city_temps.iloc[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple rows\n",
    "mult_row = city_temps.iloc[[2, 343, 10555]]\n",
    "mult_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of rows\n",
    "range_row = city_temps.iloc[502:508]\n",
    "range_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `loc` vs. `iloc`\n",
    "\n",
    "It may appear that `loc` and `iloc` are behaving exactly the same way, but that is only because the index in the `city_temps` dataframe is set to the default integer structure. Not all datasets will be like this and we can also change the index to a different column if we want (which we will learn later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Fire\n",
    "\n",
    "For selecting rows and columns simultaneously: `df.loc[row_labels, column_labels]` or `df.iloc[row_positions, column_positions]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_data = city_temps.loc[[323], ['City']]\n",
    "print(specific_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_data = city_temps.iloc[[323], [1]]\n",
    "print(specific_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple cells\n",
    "multiple_data = city_temps.loc[[323, 765], ['AverageTemperature', 'City']]\n",
    "multiple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing\n",
    "You can \"slice\" a range of rows or columns by their labels. Important: Slicing with `loc` is inclusive of the start and end label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data = city_temps.loc[5000:5005, 'AverageTemperature':'Country']\n",
    "sliced_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
